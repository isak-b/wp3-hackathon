{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydantic\n",
    "import typing\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv();\n",
    "\n",
    "import langchain.messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "# docs:\n",
    "# agent-supervisor: https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name: str, instructions: str, client):\n",
    "        self.name = name\n",
    "        self.instructions = instructions\n",
    "        self.system_message = langchain.messages.SystemMessage(instructions)\n",
    "        self.client = client\n",
    "\n",
    "    def action(self, state: MessagesState):\n",
    "        messages = [self.system_message, *state[\"messages\"]]\n",
    "        response = self.client.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def create_router(options: list[str]):\n",
    "    class Router(pydantic.BaseModel):\n",
    "        next: str = pydantic.Field(\n",
    "            description=f\"Select which node to call next: {options=}\"\n",
    "        )\n",
    "        revised_prompt: str = pydantic.Field(\n",
    "            default=\"\", description=\"Write a revised prompt to the node.\"\n",
    "        )\n",
    "\n",
    "    return Router\n",
    "\n",
    "\n",
    "class Supervisor:\n",
    "    def __init__(self, client, agents: list[str]):\n",
    "        self.name = \"Supervisor\"\n",
    "        self.client = client\n",
    "        self.agents = agents\n",
    "        self.instructions = (\n",
    "            f\"You are {self.name} tasked with managing a conversation between {self.agents}. \"\n",
    "            \"Given the following user request, respond with the name of the agent to act next. \"\n",
    "            \"You can call the agents again if you are not satisfied with the answer. \"\n",
    "            \"Try calling different agents in order to improve the answer. \"\n",
    "            \"When finished, respond with __end__.\"\n",
    "        )\n",
    "        self.system_message = langchain.messages.SystemMessage(self.instructions)\n",
    "        self.router = create_router(options=[*self.agents, \"__end__\"])\n",
    "\n",
    "        # Update the return type of .path so that the path can be graphed correctly\n",
    "        self.path.__annotations__[\"return\"] = typing.Literal[*self.agents, \"__end__\"]\n",
    "\n",
    "    def path(self, state: MessagesState):\n",
    "        return state[\"messages\"][-1].kwargs[\"next\"]\n",
    "\n",
    "    def action(self, state: MessagesState):\n",
    "        messages = [self.system_message, *state[\"messages\"]]\n",
    "        response = self.client.with_structured_output(self.router).invoke(messages)\n",
    "        message = langchain.messages.AIMessage(\n",
    "            response.revised_prompt,\n",
    "            kwargs={\"next\": response.next},\n",
    "        )\n",
    "        return {\"messages\": [message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client & Model\n",
    "client = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    base_url=os.environ[\"OPENAI_BASE_URL\"],\n",
    "    api_key=lambda: os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Agents\n",
    "agent_a = Agent(\n",
    "    name=\"AgentA\",\n",
    "    instructions=\"You are AgentA. Get the answer wrong.\",\n",
    "    client=client,\n",
    ")\n",
    "agent_b = Agent(\n",
    "    name=\"AgentB\",\n",
    "    instructions=\"You are AgentB. Answer the question.\",\n",
    "    client=client,\n",
    ")\n",
    "agents = [agent_a.name, agent_b.name]\n",
    "supervisor = Supervisor(client=client, agents=agents)\n",
    "\n",
    "# Compile graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(node=supervisor.name, action=supervisor.action)\n",
    "graph_builder.add_node(node=agent_a.name, action=agent_a.action)\n",
    "graph_builder.add_node(node=agent_b.name, action=agent_b.action)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(start_key=\"__start__\", end_key=supervisor.name)\n",
    "graph_builder.add_conditional_edges(\n",
    "    source=supervisor.name,\n",
    "    path=supervisor.path,\n",
    ")\n",
    "graph_builder.add_edge(start_key=agent_a.name, end_key=supervisor.name)\n",
    "graph_builder.add_edge(start_key=agent_b.name, end_key=supervisor.name)\n",
    "\n",
    "# Compile and display graph\n",
    "graph = graph_builder.compile()\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [langchain.messages.HumanMessage(\"What is the capital of France?\")]\n",
    "response = graph.stream(\n",
    "    {\"messages\": messages},\n",
    ")\n",
    "\n",
    "for event in response:\n",
    "    for name, payload in event.items():\n",
    "        for msg in payload[\"messages\"]:\n",
    "            if hasattr(msg, \"kwargs\") and msg.kwargs.get(\"next\"):\n",
    "                print(f\"[{name} -> {msg.kwargs.get('next')}]: {msg.content}\")\n",
    "            elif msg.content:\n",
    "                print(f\"[{name}]: {msg.content}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wp3-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
